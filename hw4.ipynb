{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Panther ID: 002615185                   HomeWork: 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Problem 1)</h1>\n",
    "<h3><strong>Object Segmentation:</strong></h3> \n",
    "Regularly we classify an image into a class. In object segmentation, we will need to classify each pixel in an image into a class.\n",
    "\n",
    "In segmentation, we're trying to determine which pixels belong to which class. We're attempting to define the fundamental boundary of meaningful items. As a result, all of the regions within the boundary should be labeled the same and placed in the same category. As a result, each pixel is assigned a semantic class, and we wish to build dense labels pixel by pixel. \n",
    "\n",
    "Assume self-driving vehicles. We're curious about the location of the street, people, pedestrians, and automobiles, among other things. In this intricate setting, we wish to make a mark on them.\n",
    "\n",
    "Let's use CNN as an example of implementation. We may also follow this line of reasoning and end up with a low-dimensional, coarse heat map for the \" automobile \" category. Of course, this is one option, but you won't be able to identify all pixels that belong to that particular class in great detail. So you'll have to figure out a way to restore the segmentation or class information to the original image resolution. The output of our CNN has to be in the exact dimensions as our input. \n",
    "\n",
    "![alt text](encoderDecoderpic.png \"Encoder Decoder\")\n",
    "\n",
    "\n",
    "<h2><strong> Encoder - Decoder: </strong></h2>\n",
    "\n",
    "The skip connections can be used to prevent, loss of information in the Decoder. The skip connections are connected from Encoder to Decoder by surpassing the bottle neck of the CNN.\n",
    "\n",
    "The decoder captures feature maps at various levels of encoded representations and concatenates them into feature maps. Aggressive pooling and downsampling in the encoder blocks of an encoder-decoder architecture help minimize data loss.\n",
    "\n",
    "\n",
    "The loss function and back propogation has to be applied to the network, based on the problem we are solving. We will need segmentation.\n",
    "\n",
    "<h2><strong>Image Segmentation in Auto Drive cars</strong></h2>\n",
    "In recent years, being able to move effectively and safely in driverless vehicles has become a popular study topic, and several firms and research centers have been working to develop the first fully functional driverless car model. This is an exciting field with numerous potential benefits, including better safety, lower prices, more comfortable travel, increased mobility, and a less environmental footprint. The process of allocating each pixel of the receiving image to one of the specified classes is known as semantic segmentation. These classes represent the image's segment names, such as roads, cars, signs, traffic signals, and pedestrians. As a result, semantic segmentation is also known as \"pixelwise categorization.\" The fundamental advantage of semantic segmentation is the ability to comprehend situations.\n",
    "\n",
    "To develop a complete picture of the driving scenario, autonomous driving relies on information from sensors in the surrounding environment. Because the visual signal is so dense with this type of data, doing semantic segmentation correctly is critical for scene comprehension. The more accurately and quickly we execute semantic segmentation, the more the ego vehicle understands the surrounding environment and makes the best decision every time. \n",
    "\n",
    "\n",
    "<h3><strong></strong><h3>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Problem 2)</h1>\n",
    "Let's see how a object detection works. This is a two stage process, which is Detecting and Classifying an object.\n",
    "\n",
    "YOLO(You only look once): As the name sugest \n",
    "\n",
    "\n",
    "![alt text](imagename.png \"Title\")\n",
    "\n",
    "\n",
    "YOLO proposes using an end-to-end neural network to make bounding box and class probability predictions all at once. The YOLO has an advantage of accuracy and speed compared to two-stage object detection. All of YOLO's predictions are made with the help of a single fully linked layer that handles the Region Proposal Network and recognition.\n",
    "\n",
    "YOLO is an algorithm that recognizes and detects different things in an image (in real-time). Object detection in YOLO is done as a regression problem, and the identified photos' class probabilities are provided. This is accomplished by using a convolutional neural network to detect objects in real-time. The key is that the system detects objects using only a single feed-forward through a neural network. \n",
    "\n",
    "The YOLO method divides the image into N grids, each with an SxS region of equal dimensions. Each of these N grids is in charge of detecting and locating the object included within it.\n",
    "These grids, in turn, estimate B bounding box coordinates about their cell coordinates, the item label, and the likelihood of the object being present in the cell.\n",
    "\n",
    "This method reduces computing time by allowing cells from the picture to handle both detection and recognition. Still, it results in many duplicate predictions due to numerous cells predicting the same object with various bounding box predictions.\n",
    "To address this problem, YOLO employs Non-Maximal Suppression.\n",
    "\n",
    "YOLO suppresses all bounding boxes with lower probability scores in Non-Maximal Suppression.\n",
    "YOLO accomplishes this by examining the probability scores linked with each decision and selecting the one with the highest score. The bounding boxes with the most prominent Intersection over Union with the current high probability bounding box are suppressed.\n",
    "This process is repeated until the bounding boxes are complete.\n",
    "\n",
    "\n",
    "<h2><strong>Faster-RCNN</strong></h2>\n",
    "\n",
    "Instead of feeding the CNN the region proposals, we give the CNN the input image to produce a convolutional feature map. We select the region of proposals from the convolutional feature map, warp them into squares, then restructure them into a fixed size using an RoI pooling layer so that they may be fed into a fully connected layer. We employ a softmax layer to forecast the class of the proposed region and the bounding box offset values from the RoI feature vector.\n",
    "Because you don't have to feed 2000 area proposals to the convolutional neural network every time, \"Fast R-CNN\" is faster than R-CNN.\n",
    "\n",
    "To locate the region proposals, the program employs selective search. Selective search is a time-consuming and slow operation that degrades network performance.\n",
    "\n",
    "The image is fed into a convolutional network, which outputs a convolutional feature map, similar to Fast R-CNN. A different network is utilized to forecast the region proposals instead of utilizing a selective search technique on the feature map to determine the region proposals. An RoI pooling layer is then used to categorize the picture within the proposed region and forecast the offset values for the bounding boxes. The projected region suggestions are reshaped.\n",
    "\n",
    "\n",
    "Major difference from to <strong>YOLO</strong> is all of the prior object detection techniques used areas to locate the object within the image. The network does not examine the entire picture. Instead, portions of the image are highly likely to contain the object. You Only Look Once, or YOLO is an object detection algorithm that differs significantly from the region-based algorithms discussed previously. A single neural network in YOLO predicts the bounding boxes and class probabilities for these boxes.\n",
    "\n",
    "YOLO is orders of magnitude faster than conventional object detection algorithms (45 frames per second). The YOLO algorithm's drawback is that it has trouble detecting small things in images; for example, it might have trouble detecting a flock of birds. This is owing to the algorithm's spatial restrictions.\n",
    "\n",
    "![alt text](yolo.png \"YOLO\")\n",
    "\n",
    "\n",
    "Bench Marking:\n",
    "\n",
    "![alt text](benchmark.png \"YOLO\")\n",
    "\n",
    "Source: https://www.researchgate.net/figure/Performance-comparison-for-Faster-RCNN-Yolo-v3-and-our-method_fig7_332102788\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
